{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03e6046",
   "metadata": {},
   "source": [
    "# This notebook is for data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c2d6d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f54628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d537af",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3d52dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16394, 14)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1882f",
   "metadata": {},
   "source": [
    "## General Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab30172b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 exact duplicate rows\n",
      "Removed 1 duplicates. New shape: (16393, 14)\n"
     ]
    }
   ],
   "source": [
    "# Removing exact duplicate rows (rows identical across all columns)\n",
    "# Count duplicates before removal\n",
    "dup_count = data.duplicated().sum()\n",
    "print(f\"Found {dup_count} exact duplicate rows\")\n",
    "if dup_count > 0:\n",
    "    # Drop exact duplicates and reset index\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Removed {dup_count} duplicates. New shape: {data.shape}\")\n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b5c95a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected ram-like columns: ['RAM_SIZE', 'RAM_TYPE']\n",
      "Detected storage-like columns (SSD/HDD): ['SSD_SIZE', 'HDD_SIZE']\n",
      "Found column mapping for grouping:\n",
      "  price -> price_preview\n",
      "  date_created -> created_at\n",
      "  etat -> spec_Etat\n",
      "  model -> model_name\n",
      "  city -> city\n",
      "  gpu -> DEDICATED_GPU\n",
      "  cpu -> CPU\n",
      "  screen_size -> RAM_SIZE\n",
      "  screen_freq -> SCREEN_SIZE\n",
      "  resolution -> SCREEN_RESOLUTION\n",
      "Using key columns for grouping: ['price_preview', 'created_at', 'spec_Etat', 'model_name', 'city', 'DEDICATED_GPU', 'CPU', 'RAM_SIZE', 'SCREEN_SIZE', 'SCREEN_RESOLUTION']\n",
      "Merged storage from index 10345 into 10344 and deleted 10345 for group (75000.0, '2024 11 01T15:06:57.000Z', 'JAMAIS UTILIS', 'VOSTRO', 'EZZOUAR', nan, '11TH GEN INTEL CORE I3 1145G4', '8GB', 14.1, nan)\n",
      "Auto-resolved 1 groups. New data shape: (16392, 14)\n",
      "Merged storage from index 10345 into 10344 and deleted 10345 for group (75000.0, '2024 11 01T15:06:57.000Z', 'JAMAIS UTILIS', 'VOSTRO', 'EZZOUAR', nan, '11TH GEN INTEL CORE I3 1145G4', '8GB', 14.1, nan)\n",
      "Auto-resolved 1 groups. New data shape: (16392, 14)\n",
      "No groups found where RAM or storage are null in some rows and not in others while other attributes match.\n",
      "Added 'id' column as primary key. Data shape now: (16392, 15)\n",
      "No groups found where RAM or storage are null in some rows and not in others while other attributes match.\n",
      "Added 'id' column as primary key. Data shape now: (16392, 15)\n"
     ]
    }
   ],
   "source": [
    "# Detect groups where all attributes except RAM or storage (SSD/HDD) match,\n",
    "# but RAM or storage is null in some rows and not in others.\n",
    "# This helps find near-duplicates where only RAM/storage information is missing in some entries\n",
    "cols = data.columns.tolist()\n",
    "# flexible matching for RAM-like columns (case-insensitive, substring match)\n",
    "ram_cols = [c for c in cols if 'ram' in c.lower() or 'memory' in c.lower()]\n",
    "# storage detection expanded to SSD, HDD, storage, drive, disk\n",
    "storage_cols = [c for c in cols if any(k in c.lower() for k in ['ssd', 'hdd', 'storage', 'drive', 'disk'])]\n",
    "print('Detected ram-like columns:', ram_cols)\n",
    "print('Detected storage-like columns (SSD/HDD):', storage_cols)\n",
    "# Group by the specific keys you requested (flexible matching):\n",
    "# price, date created, etat, model name, city, gpu, cpu, screen size, screen frequency, resolution\n",
    "def find_col(variants, cols):\n",
    "    \"\"\"Return the first column whose name contains any of the provided variants (case-insensitive).\"\"\"\n",
    "    variants = [v.lower() for v in variants]\n",
    "    for c in cols:\n",
    "        cname = c.lower()\n",
    "        if any(v in cname for v in variants):\n",
    "            return c\n",
    "    return None\n",
    "# mapping desired keys to lists of variants to match\n",
    "desired_keys = {\n",
    "    'price': ['price'],\n",
    "    'date_created': ['date', 'create', 'created'],\n",
    "    'etat': ['etat'],\n",
    "    'model': ['model', 'model name', 'model_name'],\n",
    "    'city': ['city'],\n",
    "    'gpu': ['gpu', 'graphics'],\n",
    "    'cpu': ['cpu', 'processor'],\n",
    "    'screen_size': ['screen', 'size'],\n",
    "    'screen_freq': ['frequency', 'hz', 'screen', 'refresh'],\n",
    "    'resolution': ['resolution', 'res']\n",
    "}\n",
    "found_keys = {k: find_col(v, cols) for k, v in desired_keys.items()}\n",
    "print('Found column mapping for grouping:')\n",
    "for k, col in found_keys.items():\n",
    "    print(f'  {k} -> {col}')\n",
    "# Check for missing requested columns\n",
    "missing = [k for k, col in found_keys.items() if col is None]\n",
    "if missing:\n",
    "    print('Warning: Could not find these requested grouping columns in the dataset:', missing)\n",
    "# Build key_cols from the requested keys that were found (preserve order)\n",
    "requested_order = ['price','date_created','etat','model','city','gpu','cpu','screen_size','screen_freq','resolution']\n",
    "key_cols = [found_keys[k] for k in requested_order if found_keys.get(k)]\n",
    "# If we don't have enough requested columns, fall back to grouping by all non-ram/non-storage cols\n",
    "if len(key_cols) < 2:\n",
    "    print('Not enough requested grouping columns found (need at least 2). Falling back to grouping by all non-ram/non-storage columns.')\n",
    "    key_cols = [c for c in cols if c not in ram_cols + storage_cols]\n",
    "print('Using key columns for grouping:', key_cols)\n",
    "groups = data.groupby(key_cols, dropna=False)\n",
    "problem_groups = []\n",
    "for key, grp in groups:\n",
    "    if len(grp) < 2:\n",
    "        continue\n",
    "    # check RAM columns for mixed null vs non-null within group\n",
    "    ram_issue = False\n",
    "    if ram_cols:\n",
    "        ram_issue = any(grp[rc].isna().any() and (~grp[rc].isna()).any() for rc in ram_cols)\n",
    "    # check storage columns (SSD/HDD) for mixed null vs non-null within group\n",
    "    storage_issue = False\n",
    "    if storage_cols:\n",
    "        storage_issue = any(grp[sc].isna().any() and (~grp[sc].isna()).any() for sc in storage_cols)\n",
    "    if ram_issue or storage_issue:\n",
    "        problem_groups.append((key, grp))\n",
    "# Auto-resolution: several possible strategies\n",
    "# 1) If exactly one row has any storage info and it matches '1000' (HDD 1000), copy it to others and drop source\n",
    "# 2) If multiple rows have storage info but across different storage columns (e.g., one row has SSD, another has HDD),\n",
    "#    merge storage fields into a single target row and delete the redundant row(s).\n",
    "resolved = 0\n",
    "for key, grp in problem_groups:\n",
    "    storage_existing = [c for c in storage_cols if c in grp.columns]\n",
    "    if not storage_existing:\n",
    "        continue\n",
    "    # which rows in this group have any non-null storage value\n",
    "    nonnull_mask = grp[storage_existing].notna().any(axis=1)\n",
    "    rows_with_storage = grp[nonnull_mask]\n",
    "    # Strategy A: single storage row -> maybe HDD 1000 pattern\n",
    "    if len(rows_with_storage) == 1:\n",
    "        src_idx = rows_with_storage.index[0]\n",
    "        src_vals = rows_with_storage.iloc[0][storage_existing]\n",
    "        def looks_like_1000(x):\n",
    "            if pd.isna(x):\n",
    "                return False\n",
    "            s = str(x).lower().replace(' ', '')\n",
    "            return ('1000' in s) or ('1tb' in s) or ('1000gb' in s)\n",
    "        if any(looks_like_1000(v) for v in src_vals.dropna()):\n",
    "            target_idxs = [i for i in grp.index if i != src_idx]\n",
    "            for c in storage_existing:\n",
    "                mask = data.loc[target_idxs, c].isna()\n",
    "                if mask.any():\n",
    "                    data.loc[target_idxs, c] = data.loc[target_idxs, c].where(~mask, other=data.loc[src_idx, c])\n",
    "            data.drop(index=src_idx, inplace=True)\n",
    "            resolved += 1\n",
    "            print(f'Resolved group {key}: moved storage from index {src_idx} to {target_idxs} and deleted index {src_idx}')\n",
    "        else:\n",
    "            print(f'Group {key}: single storage row found but did not match 1000 pattern; skipped auto-resolution')\n",
    "    # Strategy B: multiple rows with storage but complementary across columns -> merge into one row\n",
    "    else:\n",
    "        # determine non-null positions per storage column\n",
    "        col_nonnull_counts = {c: grp[c].notna().sum() for c in storage_existing}\n",
    "        total_nonnull_cells = sum(col_nonnull_counts.values())\n",
    "        # If non-null cells are distributed across rows without overlap (each storage cell non-null appears in only one row),\n",
    "        # we can merge them. Check that no row has two storage columns non-null (optional), and that total_nonnull_cells <= len(rows_with_storage) * len(storage_existing)\n",
    "        # Simpler heuristic: if each storage column has at most one non-null entry and the number of rows_with_storage equals the number of distinct non-null rows,\n",
    "        distinct_rows_with_storage = set(rows_with_storage.index.tolist())\n",
    "        if all(v <= 1 for v in col_nonnull_counts.values()):\n",
    "            # pick target as the first row that has any storage (prefer the one with SSD if present)\n",
    "            target_idx = None\n",
    "            # try to prefer a row that has SSD (or first storage column) non-null\n",
    "            preferred = storage_existing[0] if storage_existing else None\n",
    "            if preferred is not None:\n",
    "                candidates = rows_with_storage[rows_with_storage[preferred].notna()].index.tolist()\n",
    "                if candidates:\n",
    "                    target_idx = candidates[0]\n",
    "            if target_idx is None:\n",
    "                target_idx = rows_with_storage.index[0]\n",
    "            other_rows = [i for i in rows_with_storage.index if i != target_idx]\n",
    "            # copy complementary storage values into target where null\n",
    "            for other in other_rows:\n",
    "                for c in storage_existing:\n",
    "                    if pd.isna(data.at[target_idx, c]) and not pd.isna(data.at[other, c]):\n",
    "                        data.at[target_idx, c] = data.at[other, c]\n",
    "                # after copying, drop the other row\n",
    "                data.drop(index=other, inplace=True)\n",
    "                resolved += 1\n",
    "                print(f'Merged storage from index {other} into {target_idx} and deleted {other} for group {key}')\n",
    "        else:\n",
    "            print(f'Group {key}: storage columns have multiple non-null entries per column; skipped auto-merge')\n",
    "# After attempting auto-resolution, reset index if any changes were made\n",
    "if resolved > 0:\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    print(f'Auto-resolved {resolved} groups. New data shape: {data.shape}')\n",
    "    # Recompute groups and problem_groups on the mutated dataframe so printed results reflect changes\n",
    "    groups = data.groupby(key_cols, dropna=False)\n",
    "    new_problem_groups = []\n",
    "    for key, grp in groups:\n",
    "        if len(grp) < 2:\n",
    "            continue\n",
    "        ram_issue = False\n",
    "        if ram_cols:\n",
    "            ram_issue = any(grp[rc].isna().any() and (~grp[rc].isna()).any() for rc in ram_cols)\n",
    "        storage_issue = False\n",
    "        if storage_cols:\n",
    "            storage_issue = any(grp[sc].isna().any() and (~grp[sc].isna()).any() for sc in storage_cols)\n",
    "        if ram_issue or storage_issue:\n",
    "            new_problem_groups.append((key, grp))\n",
    "    problem_groups = new_problem_groups\n",
    "# Finally, print any remaining problem groups for manual review\n",
    "if not problem_groups:\n",
    "    print('No groups found where RAM or storage are null in some rows and not in others while other attributes match.')\n",
    "else:\n",
    "    print(f'Found {len(problem_groups)} potential groups (post-resolution):')\n",
    "    for i, (key, grp) in enumerate(problem_groups, 1):\n",
    "        print('\\n---')\n",
    "        print(f'Group {i} key: {key}')\n",
    "        # show indices and relevant columns to help decide which rows to drop\n",
    "        display_cols = key_cols + ram_cols + storage_cols\n",
    "        # ensure ordering and existence\n",
    "        display_cols = [c for c in display_cols if c in grp.columns]\n",
    "        grp_display = grp[display_cols].copy()\n",
    "        grp_display['_index'] = grp_display.index\n",
    "        print(grp_display.to_string(index=False))\n",
    "        print('---')\n",
    "\n",
    "# Add a unique integer primary key column named 'id' (starting at 1). If 'id' exists, overwrite it after warning.\n",
    "if 'id' in data.columns:\n",
    "    print(\"Column 'id' already exists in the dataset; it will be overwritten with new sequential IDs.\")\n",
    "# Ensure index is contiguous before assigning IDs\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.insert(0, 'id', range(1, len(data) + 1))\n",
    "print(f\"Added 'id' column as primary key. Data shape now: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16392, 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96c3fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported cleaned data to data_cleaned.csv with shape: (16392, 15)\n"
     ]
    }
   ],
   "source": [
    "# Export the cleaned data with IDs to CSV\n",
    "data.to_csv('data_cleaned.csv', index=False)\n",
    "print(f'Exported cleaned data to data_cleaned.csv with shape: {data.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
