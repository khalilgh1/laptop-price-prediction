{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03e6046",
   "metadata": {},
   "source": [
    "# This notebook is for data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154c2d6d",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f54628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from rapidfuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d537af",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d52dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f1882f",
   "metadata": {},
   "source": [
    "## General Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab30172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing exact duplicate rows (rows identical across all columns)\n",
    "# Count duplicates before removal\n",
    "dup_count = data.duplicated().sum()\n",
    "print(f\"Found {dup_count} exact duplicate rows\")\n",
    "if dup_count > 0:\n",
    "    # Drop exact duplicates and reset index\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    print(f\"Removed {dup_count} duplicates. New shape: {data.shape}\")\n",
    "else:\n",
    "    print('No duplicate rows found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect groups where all attributes except RAM or storage (SSD/HDD) match,\n",
    "# but RAM or storage is null in some rows and not in others.\n",
    "# This helps find near-duplicates where only RAM/storage information is missing in some entries\n",
    "cols = data.columns.tolist()\n",
    "# flexible matching for RAM-like columns (case-insensitive, substring match)\n",
    "ram_cols = [c for c in cols if 'ram' in c.lower() or 'memory' in c.lower()]\n",
    "# storage detection expanded to SSD, HDD, storage, drive, disk\n",
    "storage_cols = [c for c in cols if any(k in c.lower() for k in ['ssd', 'hdd', 'storage', 'drive', 'disk'])]\n",
    "print('Detected ram-like columns:', ram_cols)\n",
    "print('Detected storage-like columns (SSD/HDD):', storage_cols)\n",
    "# Group by the specific keys you requested (flexible matching):\n",
    "# price, date created, etat, model name, city, gpu, cpu, screen size, screen frequency, resolution\n",
    "def find_col(variants, cols):\n",
    "    \"\"\"Return the first column whose name contains any of the provided variants (case-insensitive).\"\"\"\n",
    "    variants = [v.lower() for v in variants]\n",
    "    for c in cols:\n",
    "        cname = c.lower()\n",
    "        if any(v in cname for v in variants):\n",
    "            return c\n",
    "    return None\n",
    "# mapping desired keys to lists of variants to match\n",
    "desired_keys = {\n",
    "    'price': ['price'],\n",
    "    'date_created': ['date', 'create', 'created'],\n",
    "    'etat': ['etat'],\n",
    "    'model': ['model', 'model name', 'model_name'],\n",
    "    'city': ['city'],\n",
    "    'gpu': ['gpu', 'graphics'],\n",
    "    'cpu': ['cpu', 'processor'],\n",
    "    'screen_size': ['screen', 'size'],\n",
    "    'screen_freq': ['frequency', 'hz', 'screen', 'refresh'],\n",
    "    'resolution': ['resolution', 'res']\n",
    "}\n",
    "found_keys = {k: find_col(v, cols) for k, v in desired_keys.items()}\n",
    "print('Found column mapping for grouping:')\n",
    "for k, col in found_keys.items():\n",
    "    print(f'  {k} -> {col}')\n",
    "# Check for missing requested columns\n",
    "missing = [k for k, col in found_keys.items() if col is None]\n",
    "if missing:\n",
    "    print('Warning: Could not find these requested grouping columns in the dataset:', missing)\n",
    "# Build key_cols from the requested keys that were found (preserve order)\n",
    "requested_order = ['price','date_created','etat','model','city','gpu','cpu','screen_size','screen_freq','resolution']\n",
    "key_cols = [found_keys[k] for k in requested_order if found_keys.get(k)]\n",
    "# If we don't have enough requested columns, fall back to grouping by all non-ram/non-storage cols\n",
    "if len(key_cols) < 2:\n",
    "    print('Not enough requested grouping columns found (need at least 2). Falling back to grouping by all non-ram/non-storage columns.')\n",
    "    key_cols = [c for c in cols if c not in ram_cols + storage_cols]\n",
    "print('Using key columns for grouping:', key_cols)\n",
    "groups = data.groupby(key_cols, dropna=False)\n",
    "problem_groups = []\n",
    "for key, grp in groups:\n",
    "    if len(grp) < 2:\n",
    "        continue\n",
    "    # check RAM columns for mixed null vs non-null within group\n",
    "    ram_issue = False\n",
    "    if ram_cols:\n",
    "        ram_issue = any(grp[rc].isna().any() and (~grp[rc].isna()).any() for rc in ram_cols)\n",
    "    # check storage columns (SSD/HDD) for mixed null vs non-null within group\n",
    "    storage_issue = False\n",
    "    if storage_cols:\n",
    "        storage_issue = any(grp[sc].isna().any() and (~grp[sc].isna()).any() for sc in storage_cols)\n",
    "    if ram_issue or storage_issue:\n",
    "        problem_groups.append((key, grp))\n",
    "# Auto-resolution: several possible strategies\n",
    "# 1) If exactly one row has any storage info and it matches '1000' (HDD 1000), copy it to others and drop source\n",
    "# 2) If multiple rows have storage info but across different storage columns (e.g., one row has SSD, another has HDD),\n",
    "#    merge storage fields into a single target row and delete the redundant row(s).\n",
    "resolved = 0\n",
    "for key, grp in problem_groups:\n",
    "    storage_existing = [c for c in storage_cols if c in grp.columns]\n",
    "    if not storage_existing:\n",
    "        continue\n",
    "    # which rows in this group have any non-null storage value\n",
    "    nonnull_mask = grp[storage_existing].notna().any(axis=1)\n",
    "    rows_with_storage = grp[nonnull_mask]\n",
    "    # Strategy A: single storage row -> maybe HDD 1000 pattern\n",
    "    if len(rows_with_storage) == 1:\n",
    "        src_idx = rows_with_storage.index[0]\n",
    "        src_vals = rows_with_storage.iloc[0][storage_existing]\n",
    "        def looks_like_1000(x):\n",
    "            if pd.isna(x):\n",
    "                return False\n",
    "            s = str(x).lower().replace(' ', '')\n",
    "            return ('1000' in s) or ('1tb' in s) or ('1000gb' in s)\n",
    "        if any(looks_like_1000(v) for v in src_vals.dropna()):\n",
    "            target_idxs = [i for i in grp.index if i != src_idx]\n",
    "            for c in storage_existing:\n",
    "                mask = data.loc[target_idxs, c].isna()\n",
    "                if mask.any():\n",
    "                    data.loc[target_idxs, c] = data.loc[target_idxs, c].where(~mask, other=data.loc[src_idx, c])\n",
    "            data.drop(index=src_idx, inplace=True)\n",
    "            resolved += 1\n",
    "            print(f'Resolved group {key}: moved storage from index {src_idx} to {target_idxs} and deleted index {src_idx}')\n",
    "        else:\n",
    "            print(f'Group {key}: single storage row found but did not match 1000 pattern; skipped auto-resolution')\n",
    "    # Strategy B: multiple rows with storage but complementary across columns -> merge into one row\n",
    "    else:\n",
    "        # determine non-null positions per storage column\n",
    "        col_nonnull_counts = {c: grp[c].notna().sum() for c in storage_existing}\n",
    "        total_nonnull_cells = sum(col_nonnull_counts.values())\n",
    "        # If non-null cells are distributed across rows without overlap (each storage cell non-null appears in only one row),\n",
    "        # we can merge them. Check that no row has two storage columns non-null (optional), and that total_nonnull_cells <= len(rows_with_storage) * len(storage_existing)\n",
    "        # Simpler heuristic: if each storage column has at most one non-null entry and the number of rows_with_storage equals the number of distinct non-null rows,\n",
    "        distinct_rows_with_storage = set(rows_with_storage.index.tolist())\n",
    "        if all(v <= 1 for v in col_nonnull_counts.values()):\n",
    "            # pick target as the first row that has any storage (prefer the one with SSD if present)\n",
    "            target_idx = None\n",
    "            # try to prefer a row that has SSD (or first storage column) non-null\n",
    "            preferred = storage_existing[0] if storage_existing else None\n",
    "            if preferred is not None:\n",
    "                candidates = rows_with_storage[rows_with_storage[preferred].notna()].index.tolist()\n",
    "                if candidates:\n",
    "                    target_idx = candidates[0]\n",
    "            if target_idx is None:\n",
    "                target_idx = rows_with_storage.index[0]\n",
    "            other_rows = [i for i in rows_with_storage.index if i != target_idx]\n",
    "            # copy complementary storage values into target where null\n",
    "            for other in other_rows:\n",
    "                for c in storage_existing:\n",
    "                    if pd.isna(data.at[target_idx, c]) and not pd.isna(data.at[other, c]):\n",
    "                        data.at[target_idx, c] = data.at[other, c]\n",
    "                # after copying, drop the other row\n",
    "                data.drop(index=other, inplace=True)\n",
    "                resolved += 1\n",
    "                print(f'Merged storage from index {other} into {target_idx} and deleted {other} for group {key}')\n",
    "        else:\n",
    "            print(f'Group {key}: storage columns have multiple non-null entries per column; skipped auto-merge')\n",
    "# After attempting auto-resolution, reset index if any changes were made\n",
    "if resolved > 0:\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    print(f'Auto-resolved {resolved} groups. New data shape: {data.shape}')\n",
    "    # Recompute groups and problem_groups on the mutated dataframe so printed results reflect changes\n",
    "    groups = data.groupby(key_cols, dropna=False)\n",
    "    new_problem_groups = []\n",
    "    for key, grp in groups:\n",
    "        if len(grp) < 2:\n",
    "            continue\n",
    "        ram_issue = False\n",
    "        if ram_cols:\n",
    "            ram_issue = any(grp[rc].isna().any() and (~grp[rc].isna()).any() for rc in ram_cols)\n",
    "        storage_issue = False\n",
    "        if storage_cols:\n",
    "            storage_issue = any(grp[sc].isna().any() and (~grp[sc].isna()).any() for sc in storage_cols)\n",
    "        if ram_issue or storage_issue:\n",
    "            new_problem_groups.append((key, grp))\n",
    "    problem_groups = new_problem_groups\n",
    "# Finally, print any remaining problem groups for manual review\n",
    "if not problem_groups:\n",
    "    print('No groups found where RAM or storage are null in some rows and not in others while other attributes match.')\n",
    "else:\n",
    "    print(f'Found {len(problem_groups)} potential groups (post-resolution):')\n",
    "    for i, (key, grp) in enumerate(problem_groups, 1):\n",
    "        print('\\n---')\n",
    "        print(f'Group {i} key: {key}')\n",
    "        # show indices and relevant columns to help decide which rows to drop\n",
    "        display_cols = key_cols + ram_cols + storage_cols\n",
    "        # ensure ordering and existence\n",
    "        display_cols = [c for c in display_cols if c in grp.columns]\n",
    "        grp_display = grp[display_cols].copy()\n",
    "        grp_display['_index'] = grp_display.index\n",
    "        print(grp_display.to_string(index=False))\n",
    "        print('---')\n",
    "\n",
    "# Add a unique integer primary key column named 'id' (starting at 1). If 'id' exists, overwrite it after warning.\n",
    "if 'id' in data.columns:\n",
    "    print(\"Column 'id' already exists in the dataset; it will be overwritten with new sequential IDs.\")\n",
    "# Ensure index is contiguous before assigning IDs\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data.insert(0, 'id', range(1, len(data) + 1))\n",
    "print(f\"Added 'id' column as primary key. Data shape now: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the cleaned data with IDs to CSV\n",
    "data.to_csv('data_cleaned.csv', index=False)\n",
    "print(f'Exported cleaned data to data_cleaned.csv with shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0a17a",
   "metadata": {},
   "source": [
    "## Merge cpu data into main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus_data = pd.read_csv('cpus.csv', on_bad_lines='warn')\n",
    "\n",
    "# ------------------ NORMALIZATION ------------------\n",
    "\n",
    "def normalize(s):\n",
    "    if not s or pd.isna(s):\n",
    "        return ''\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'intel|processor|core|cpu', '', s)\n",
    "    s = s.replace('-', ' ')\n",
    "    s = re.sub(r'[^a-z0-9 ]+', ' ', s)\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "# ------------------ TYPO/NEAR-MATCH CORRECTIONS ------------------\n",
    "\n",
    "# Common typos and incomplete model numbers -> correct model\n",
    "CPU_CORRECTIONS = {\n",
    "    # 11th Gen typos (missing G7 suffix or wrong letter)\n",
    "    'i5 1135u': 'i5-1135G7',\n",
    "    'i5 1135': 'i5-1135G7',\n",
    "    'i3 1115g7': 'i3-1115G4',  # G7 doesn't exist, it's G4\n",
    "    'i3 1124g': 'i3-1125G4',   # typo\n",
    "    'i3 1134g4': 'i3-1115G4',  # doesn't exist\n",
    "    'i3 1145g4': 'i3-1115G4',  # doesn't exist\n",
    "    \n",
    "    # 12th Gen\n",
    "    'i5 1244u': 'i5-1245U',    # typo\n",
    "    'i5 1285p': 'i5-1240P',    # doesn't exist, closest is 1240P\n",
    "    'i5 1235p': 'i5-1240P',    # P series\n",
    "    'i5 12210u': 'i5-1235U',   # typo\n",
    "    \n",
    "    # 13th Gen\n",
    "    'i7 13350u': 'i7-1355U',   # doesn't exist\n",
    "    'i7 13340u': 'i7-1355U',   # doesn't exist\n",
    "    'i7 1365p': 'i7-1360P',    # typo\n",
    "    'i5 1345p': 'i5-1340P',    # typo\n",
    "    \n",
    "    # 8th Gen\n",
    "    'i5 8300u': 'i5-8250U',    # 8300U doesn't exist\n",
    "    'i5 8700': 'i5-8300H',     # desktop CPU, map to laptop equivalent\n",
    "    'i5 8265': 'i5-8265U',     # missing U\n",
    "    'i5 8600': 'i5-8300H',     # desktop\n",
    "    'i5 8350 vpro': 'i5-8350U',\n",
    "    'i5 8300 vpro': 'i5-8250U',\n",
    "    'i5 8350de': 'i5-8350U',   # typo\n",
    "    'i5 8635u': 'i5-8265U',    # doesn't exist\n",
    "    'i7 8560u': 'i7-8550U',    # typo\n",
    "    \n",
    "    # 7th Gen\n",
    "    'i5 7300': 'i5-7300U',     # missing suffix\n",
    "    'i5 7300 vpro': 'i5-7300U',\n",
    "    'i5 7400u': 'i5-7200U',    # 7400U doesn't exist\n",
    "    'i5 7400': 'i5-7300HQ',    # desktop, map to laptop\n",
    "    'i7 7375u': 'i7-7500U',    # doesn't exist\n",
    "    \n",
    "    # 6th Gen\n",
    "    'i5 6300': 'i5-6300U',     # missing suffix\n",
    "    'i7 6600': 'i7-6600U',     # missing suffix\n",
    "    'i7 6600hq': 'i7-6700HQ',  # typo\n",
    "    'i7 6850hq': 'i7-6820HQ',  # typo\n",
    "    'i7 6550u': 'i7-6500U',    # typo\n",
    "    'i3 6006': 'i3-6006U',     # missing suffix\n",
    "    \n",
    "    # 4th Gen\n",
    "    'i7 4712': 'i7-4712MQ',    # missing suffix\n",
    "    'i5 4570m': 'i5-4200M',    # 4570M doesn't exist\n",
    "    'i3 4050u': 'i3-4030U',    # typo\n",
    "    \n",
    "    # 3rd Gen\n",
    "    'i3 3220': 'i3-3120M',     # desktop, map to laptop\n",
    "    'i3 3300': 'i3-3120M',     # doesn't exist\n",
    "    \n",
    "    # 2nd Gen\n",
    "    'i5 2415m': 'i5-2410M',    # typo\n",
    "    \n",
    "    # 9th Gen\n",
    "    'i7 9900': 'i7-9750H',     # desktop, map to laptop\n",
    "    \n",
    "    # Core M series\n",
    "    'm3 7e': 'Core m3-7Y30',   # incomplete\n",
    "    \n",
    "    # Intel N-series\n",
    "    'n200': 'Intel N200',\n",
    "    'n4500': 'Intel Celeron N4500',\n",
    "    \n",
    "    # Misc\n",
    "    '620': 'Intel Core 2 Duo T6600',  # very old, guess\n",
    "}\n",
    "\n",
    "def apply_cpu_corrections(normalized_cpu, original_cpu):\n",
    "    \"\"\"Apply known corrections for typos and incomplete model numbers.\"\"\"\n",
    "    # Check direct match\n",
    "    if normalized_cpu in CPU_CORRECTIONS:\n",
    "        return CPU_CORRECTIONS[normalized_cpu]\n",
    "    \n",
    "    # Check with gen prefix removed\n",
    "    no_gen = re.sub(r'^\\d+(?:th|nd|rd|st)?\\s*gen\\s*', '', normalized_cpu)\n",
    "    if no_gen in CPU_CORRECTIONS:\n",
    "        return CPU_CORRECTIONS[no_gen]\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# ------------------ GENERIC CPU DETECTION ------------------\n",
    "\n",
    "def detect_generic_cpu(cpu_name):\n",
    "    \"\"\"\n",
    "    Detect generic CPU names like '11TH GEN INTEL CORE I5' or 'AMD RYZEN 5'\n",
    "    Returns a tuple: (is_generic, brand, tier, generation) or (False, None, None, None)\n",
    "    \"\"\"\n",
    "    if not cpu_name or pd.isna(cpu_name):\n",
    "        return (False, None, None, None)\n",
    "    \n",
    "    s = str(cpu_name).lower().strip()\n",
    "    \n",
    "    # Skip Apple processors (not AMD A-series!)\n",
    "    if 'apple' in s or 'bionic' in s or 'm1' in s or 'm2' in s or 'm3' in s:\n",
    "        return (False, None, None, None)\n",
    "    \n",
    "    # Skip Intel Core Ultra (not AMD A-series!)\n",
    "    if 'ultra' in s:\n",
    "        # Intel Core Ultra - handle separately\n",
    "        ultra_match = re.search(r'ultra\\s*(\\d)', s)\n",
    "        if ultra_match:\n",
    "            tier = f\"ultra {ultra_match.group(1)}\"\n",
    "            # Check if it has a specific model number\n",
    "            has_model = re.search(r'ultra\\s*\\d\\s+\\d{3}', s)\n",
    "            if not has_model:\n",
    "                return (True, 'intel', tier, None)\n",
    "        return (False, None, None, None)\n",
    "    \n",
    "    # Intel N-series (N100, N200, N4500, N5100, etc.)\n",
    "    if re.search(r'\\bn[0-9]{3,4}\\b', s):\n",
    "        return (False, None, None, None)  # These are specific models, not generic\n",
    "    \n",
    "    # Intel patterns: \"11th gen intel core i5\", \"intel core i7 12th gen\", \"core i5 11th\"\n",
    "    intel_pattern = re.search(\n",
    "        r'(?:(\\d{1,2})(?:th|nd|rd|st)?\\s*gen(?:eration)?)?'  # optional gen prefix\n",
    "        r'.*?(?:intel)?.*?(?:core)?\\s*'\n",
    "        r'(i[3579]|celeron|pentium|atom)'  # tier\n",
    "        r'(?:\\s*(\\d{1,2})(?:th|nd|rd|st)?\\s*gen(?:eration)?)?',  # optional gen suffix\n",
    "        s\n",
    "    )\n",
    "    \n",
    "    if intel_pattern:\n",
    "        gen_prefix = intel_pattern.group(1)\n",
    "        tier = intel_pattern.group(2)\n",
    "        gen_suffix = intel_pattern.group(3)\n",
    "        generation = gen_prefix or gen_suffix\n",
    "        \n",
    "        # Check if this is truly generic (no specific model number like 1135G7)\n",
    "        # Generic: \"11th gen i5\", Non-generic: \"i5-1135G7\" or \"i5 1135G7\"\n",
    "        has_model = re.search(r'i[3579]\\s*[-]?\\s*\\d{4,5}', s)\n",
    "        \n",
    "        # For Pentium/Celeron, check for model numbers differently\n",
    "        if tier in ['pentium', 'celeron']:\n",
    "            has_model = re.search(r'(pentium|celeron)\\s*(gold|silver)?\\s*[a-z]?\\d{4}', s)\n",
    "        \n",
    "        if tier and not has_model:\n",
    "            return (True, 'intel', tier, generation)\n",
    "    \n",
    "    # AMD patterns: \"AMD Ryzen 5\", \"Ryzen 7 5000 series\", \"AMD Ryzen 5 5000\"\n",
    "    # But NOT \"AMD Ryzen 8845\" which has a partial model number\n",
    "    amd_pattern = re.search(\n",
    "        r'(?:amd)?\\s*ryzen\\s*(\\d)'  # Ryzen tier (3, 5, 7, 9)\n",
    "        r'(?:\\s*(\\d{4})(?:\\s*series)?)?',  # optional series like 5000, 7000\n",
    "        s\n",
    "    )\n",
    "    \n",
    "    if amd_pattern:\n",
    "        tier = f\"ryzen {amd_pattern.group(1)}\"\n",
    "        series = amd_pattern.group(2)\n",
    "        \n",
    "        # Check if this is truly generic (no specific model like 5600H or even partial 8845)\n",
    "        # Match patterns like \"ryzen 5 5600H\" or \"ryzen 8845\"\n",
    "        has_model = re.search(r'ryzen\\s*\\d\\s+\\d{4}[a-z]*', s)\n",
    "        # Also check for pattern like \"ryzen 8845\" (no tier number)\n",
    "        partial_model = re.search(r'ryzen\\s+\\d{4}', s)\n",
    "        \n",
    "        if has_model or partial_model:\n",
    "            return (False, None, None, None)  # Not generic, has model number\n",
    "        \n",
    "        return (True, 'amd', tier, series)\n",
    "    \n",
    "    # AMD A-series: \"AMD A10\", \"A6\" - but NOT Apple or Intel Ultra\n",
    "    # Must have explicit \"amd\" or \"a\" followed by number without other context\n",
    "    amd_a_pattern = re.search(r'(?:^|\\s)(?:amd\\s+)?(a\\d{1,2})(?:\\s|$|-)', s)\n",
    "    if amd_a_pattern and 'intel' not in s and 'apple' not in s and 'ultra' not in s:\n",
    "        tier = amd_a_pattern.group(1)\n",
    "        has_model = re.search(r'a\\d{1,2}\\s*[-]?\\s*\\d{4}', s)\n",
    "        if not has_model:\n",
    "            return (True, 'amd', tier, None)\n",
    "    \n",
    "    return (False, None, None, None)\n",
    "\n",
    "\n",
    "def get_generic_cpu_stats(brand, tier, generation, cpus_df):\n",
    "    \"\"\"\n",
    "    Get average stats for a generic CPU specification by filtering matching CPUs.\n",
    "    \"\"\"\n",
    "    df = cpus_df.copy()\n",
    "    df['name_lower'] = df['name'].str.lower()\n",
    "    \n",
    "    if brand == 'intel':\n",
    "        if 'ultra' in tier:\n",
    "            # Intel Core Ultra series\n",
    "            ultra_num = tier.split()[-1]  # \"ultra 7\" -> \"7\"\n",
    "            mask = df['name_lower'].str.contains(f'ultra\\\\s*{ultra_num}', regex=True, na=False)\n",
    "        elif tier in ['pentium', 'celeron', 'atom']:\n",
    "            # For Pentium/Celeron/Atom, just filter by name\n",
    "            mask = df['name_lower'].str.contains(tier, regex=False, na=False)\n",
    "            # Also filter to laptop category\n",
    "            if 'cat' in df.columns:\n",
    "                mask = mask & (df['cat'].str.lower() == 'laptop')\n",
    "        else:\n",
    "            # Core i3/i5/i7/i9\n",
    "            tier_pattern = tier.replace('i', 'i[- ]?')  # handle i5, i-5, i 5\n",
    "            mask = df['name_lower'].str.contains(tier_pattern, regex=True, na=False)\n",
    "            \n",
    "            # Filter by generation if specified\n",
    "            if generation:\n",
    "                gen = int(generation)\n",
    "                if gen <= 10:\n",
    "                    # Older gens: look for patterns like \"i5-8250U\" (8th gen starts with 8)\n",
    "                    gen_pattern = rf'{tier_pattern}\\s*[-]?\\s*{gen}\\d{{3}}'\n",
    "                    mask = mask & df['name_lower'].str.contains(gen_pattern, regex=True, na=False)\n",
    "                else:\n",
    "                    # 11th gen+: look for patterns like \"i5-1135G7\" (11th gen starts with 11)\n",
    "                    gen_pattern = rf'{tier_pattern}\\s*[-]?\\s*{gen}\\d{{2}}'\n",
    "                    mask = mask & df['name_lower'].str.contains(gen_pattern, regex=True, na=False)\n",
    "    \n",
    "    elif brand == 'amd':\n",
    "        if 'ryzen' in tier:\n",
    "            ryzen_num = tier.split()[-1]  # \"ryzen 5\" -> \"5\"\n",
    "            mask = df['name_lower'].str.contains(f'ryzen\\\\s*{ryzen_num}', regex=True, na=False)\n",
    "            \n",
    "            # Filter by series if specified (5000, 7000, etc.)\n",
    "            if generation:\n",
    "                series = str(generation)\n",
    "                # Match CPUs starting with that series number (5600, 5800, 7530, etc.)\n",
    "                series_pattern = rf'ryzen\\s*{ryzen_num}\\s+{series[0]}\\d{{3}}'\n",
    "                mask = mask & df['name_lower'].str.contains(series_pattern, regex=True, na=False)\n",
    "        else:\n",
    "            # AMD A-series\n",
    "            mask = df['name_lower'].str.contains(f'{tier}[-\\\\s]', regex=True, na=False)\n",
    "    else:\n",
    "        mask = pd.Series([False] * len(df))\n",
    "    \n",
    "    filtered = df[mask]\n",
    "    \n",
    "    if len(filtered) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Calculate median stats (more robust than mean for outliers)\n",
    "    # Handle cpumark which might have commas\n",
    "    def parse_cpumark(val):\n",
    "        if pd.isna(val):\n",
    "            return np.nan\n",
    "        return float(str(val).replace(',', ''))\n",
    "    \n",
    "    cpumarks = filtered['cpumark'].apply(parse_cpumark)\n",
    "    \n",
    "    tdp_col = 'tdp(W)' if 'tdp(W)' in filtered.columns else 'tdp'\n",
    "    \n",
    "    # Get most common gpu_name for this tier\n",
    "    if 'gpu_name' in filtered.columns:\n",
    "        gpu_counts = filtered['gpu_name'].value_counts()\n",
    "        common_gpu = gpu_counts.index[0] if len(gpu_counts) > 0 else 'NA'\n",
    "    else:\n",
    "        common_gpu = 'NA'\n",
    "    \n",
    "    return {\n",
    "        'cpu_name': f\"Generic {brand.upper()} {tier.upper()}\" + (f\" {generation}th Gen\" if generation else \"\"),\n",
    "        'cores': int(filtered['cores'].median()) if filtered['cores'].notna().any() else 'NA',\n",
    "        'cpu_mark': int(cpumarks.median()) if cpumarks.notna().any() else 'NA',\n",
    "        'tdp': round(filtered[tdp_col].median(), 1) if filtered[tdp_col].notna().any() else 'NA',\n",
    "        'gpu_name': common_gpu,\n",
    "        'match_count': len(filtered)\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------ PREPARE CLEANED CPUS ------------------\n",
    "\n",
    "# Create normalized CPU names from cpus_data\n",
    "cpus_data['norm'] = cpus_data['name'].apply(normalize)\n",
    "\n",
    "# Get column names (handle different possible column names for tdp)\n",
    "tdp_col = 'tdp(W)' if 'tdp(W)' in cpus_data.columns else 'tdp'\n",
    "\n",
    "cpus = cpus_data[['name', 'norm', 'cores', 'cpumark', tdp_col]].copy()\n",
    "cpus.columns = ['cpu_name', 'norm', 'cores', 'cpu_mark', 'tdp']\n",
    "cpu_norms = cpus['norm'].tolist()\n",
    "\n",
    "# Also create a dict for fast lookup by name\n",
    "cpu_by_name = {row['cpu_name'].lower(): idx for idx, row in cpus.iterrows()}\n",
    "\n",
    "# ------------------ NORMALIZE DATA CPU NAMES ------------------\n",
    "\n",
    "# Find the CPU column in data\n",
    "cpu_col = None\n",
    "for col in ['cpu_name', 'CPU', 'cpu', 'Cpu']:\n",
    "    if col in data.columns:\n",
    "        cpu_col = col\n",
    "        break\n",
    "\n",
    "if cpu_col is None:\n",
    "    raise ValueError(\"No CPU column found in data\")\n",
    "\n",
    "data['norm_cpu'] = data[cpu_col].apply(normalize)\n",
    "\n",
    "# ------------------ MATCH & MAP ------------------\n",
    "\n",
    "MATCH_THRESHOLD = 60\n",
    "\n",
    "matched = unmatched = generic_matched = corrected = 0\n",
    "scores = []\n",
    "results = []\n",
    "\n",
    "for idx, row in data.iterrows():\n",
    "    original_cpu = row[cpu_col]\n",
    "    n = row['norm_cpu']\n",
    "    \n",
    "    if not n:\n",
    "        results.append({\n",
    "            'mapped_cpu_name': 'NA',\n",
    "            'match_score': 0,\n",
    "            'cores': 'NA',\n",
    "            'cpu_mark': 'NA',\n",
    "            'tdp': 'NA',\n",
    "            'gpu_name': 'NA',\n",
    "            'match_type': 'empty'\n",
    "        })\n",
    "        unmatched += 1\n",
    "        scores.append(0)\n",
    "        continue\n",
    "\n",
    "    # Step 1: Check for known typos/corrections\n",
    "    correction = apply_cpu_corrections(n, original_cpu)\n",
    "    if correction:\n",
    "        # Try to find the corrected CPU in our database\n",
    "        correction_norm = normalize(correction)\n",
    "        match = process.extractOne(correction_norm, cpu_norms, scorer=fuzz.token_set_ratio)\n",
    "        if match and match[1] >= 80:  # Higher threshold for corrections\n",
    "            _, score, match_idx = match\n",
    "            cpu = cpus.iloc[match_idx]\n",
    "            results.append({\n",
    "                'mapped_cpu_name': cpu['cpu_name'],\n",
    "                'match_score': score,\n",
    "                'cores': cpu['cores'],\n",
    "                'cpu_mark': cpu['cpu_mark'],\n",
    "                'tdp': cpu['tdp'],\n",
    "                'gpu_name': cpus_data.iloc[match_idx].get('gpu_name', 'NA'),\n",
    "                'match_type': f'corrected ({original_cpu} -> {correction})'\n",
    "            })\n",
    "            corrected += 1\n",
    "            scores.append(score)\n",
    "            continue\n",
    "\n",
    "    # Step 2: Check if this is a generic CPU name\n",
    "    is_generic, brand, tier, generation = detect_generic_cpu(original_cpu)\n",
    "    \n",
    "    if is_generic:\n",
    "        generic_stats = get_generic_cpu_stats(brand, tier, generation, cpus_data)\n",
    "        \n",
    "        if generic_stats:\n",
    "            results.append({\n",
    "                'mapped_cpu_name': generic_stats['cpu_name'],\n",
    "                'match_score': 100,  # Perfect match for generic\n",
    "                'cores': generic_stats['cores'],\n",
    "                'cpu_mark': generic_stats['cpu_mark'],\n",
    "                'tdp': generic_stats['tdp'],\n",
    "                'gpu_name': generic_stats['gpu_name'],\n",
    "                'match_type': f'generic (based on {generic_stats[\"match_count\"]} CPUs)'\n",
    "            })\n",
    "            generic_matched += 1\n",
    "            scores.append(100)\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Generic CPU detected but no matching CPUs found: \"{original_cpu}\" ({brand} {tier} gen {generation})')\n",
    "\n",
    "    # Step 3: Standard fuzzy matching\n",
    "    match = process.extractOne(\n",
    "        n,\n",
    "        cpu_norms,\n",
    "        scorer=fuzz.token_set_ratio\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "        _, score, match_idx = match\n",
    "        scores.append(score)\n",
    "\n",
    "        if score >= MATCH_THRESHOLD:\n",
    "            cpu = cpus.iloc[match_idx]\n",
    "            results.append({\n",
    "                'mapped_cpu_name': cpu['cpu_name'],\n",
    "                'match_score': score,\n",
    "                'cores': cpu['cores'],\n",
    "                'cpu_mark': cpu['cpu_mark'],\n",
    "                'tdp': cpu['tdp'],\n",
    "                'gpu_name': cpus_data.iloc[match_idx].get('gpu_name', 'NA'),\n",
    "                'match_type': 'fuzzy'\n",
    "            })\n",
    "            matched += 1\n",
    "        else:\n",
    "            # Let's print the cpu name that didn't match well\n",
    "            print(f'Unmatched CPU (score {score}): \"{original_cpu}\" normalized as \"{n}\"')\n",
    "            results.append({\n",
    "                'mapped_cpu_name': 'NA',\n",
    "                'match_score': score,\n",
    "                'cores': 'NA',\n",
    "                'cpu_mark': 'NA',\n",
    "                'tdp': 'NA',\n",
    "                'gpu_name': 'NA',\n",
    "                'match_type': 'unmatched'\n",
    "            })\n",
    "            unmatched += 1\n",
    "    else:\n",
    "        results.append({\n",
    "            'mapped_cpu_name': 'NA',\n",
    "            'match_score': 0,\n",
    "            'cores': 'NA',\n",
    "            'cpu_mark': 'NA',\n",
    "            'tdp': 'NA',\n",
    "            'gpu_name': 'NA',\n",
    "            'match_type': 'no_match'\n",
    "        })\n",
    "        unmatched += 1\n",
    "        scores.append(0)\n",
    "\n",
    "# ------------------ MERGE RESULTS ------------------\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "data_merged = pd.concat([data.reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "# Drop temporary column\n",
    "data_merged = data_merged.drop(columns=['norm_cpu'])\n",
    "\n",
    "# ------------------ SAVE OUTPUT ------------------\n",
    "\n",
    "OUT_FN = 'data_with_cpus.csv'\n",
    "data_merged.to_csv(OUT_FN, index=False)\n",
    "\n",
    "# ------------------ REPORT ------------------\n",
    "\n",
    "total = len(data)\n",
    "avg_score = sum(scores) / len(scores) if scores else 0\n",
    "print(\n",
    "    f'\\nWrote {OUT_FN} ({total} rows). '\n",
    "    f'Fuzzy: {matched}, Generic: {generic_matched}, Corrected: {corrected}, Unmatched: {unmatched}, '\n",
    "    f'Avg score: {avg_score:.1f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99324a1b",
   "metadata": {},
   "source": [
    "## Add gpu data to main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3aa040",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(OUT_FN)\n",
    "\n",
    "# Load cleaned GPUs reference\n",
    "gpus_ref = pd.read_csv('gpus.csv')\n",
    "\n",
    "# Normalization function for GPU names (same as in tools/map_gpus.py)\n",
    "def normalize_gpu(s):\n",
    "    if not s or pd.isna(s):\n",
    "        return ''\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(r'\\b(nvidia|geforce|radeon|radeon pro|intel|graphics|gpu|mobile|laptop|pc|mx|with|max-q|ti|gtx|rtx|series|apple)\\b', '', s)\n",
    "    s = s.replace('-', ' ')\n",
    "    s = re.sub(r'[^a-z0-9 ]+', ' ', s)\n",
    "    return re.sub(r'\\s+', ' ', s).strip()\n",
    "\n",
    "# Build normalized GPU lookup\n",
    "gpu_norms = gpus_ref['gpu_name'].apply(normalize_gpu).tolist()\n",
    "gpu_data = gpus_ref.to_dict('records')\n",
    "\n",
    "# Create a lookup dict by exact gpu_name for faster access\n",
    "gpu_by_name = {g['gpu_name'].lower(): g for g in gpu_data}\n",
    "\n",
    "# Best match function using rapidfuzz\n",
    "def best_gpu_match(query, choices):\n",
    "    if not query:\n",
    "        return None\n",
    "    match = process.extractOne(query, choices, scorer=fuzz.token_set_ratio)\n",
    "    if match:\n",
    "        return match  # (choice, score, idx)\n",
    "    return None\n",
    "\n",
    "# Apple GPU mapping based on CPU type\n",
    "APPLE_GPU_MAP = {\n",
    "    # M1 series - 8-core GPU (closest to 19-core performance tier)\n",
    "    'm1': 'Apple 19-core GPU',\n",
    "    'm1 pro': 'Apple 19-core GPU',\n",
    "    'm1 max': 'Apple 38-core GPU',\n",
    "    'm1 ultra': 'Apple 64-core GPU',\n",
    "    # M2 series\n",
    "    'm2': 'Apple 19-core GPU',\n",
    "    'm2 pro': 'Apple 19-core GPU',\n",
    "    'm2 max': 'Apple 38-core GPU',\n",
    "    'm2 ultra': 'Apple 76-core GPU',\n",
    "    # M3 series\n",
    "    'm3': 'Apple 19-core GPU',\n",
    "    'm3 pro': 'Apple 19-core GPU',\n",
    "    'm3 max': 'Apple 38-core GPU',\n",
    "    # M4 series\n",
    "    'm4': 'Apple 19-core GPU',\n",
    "    'm4 pro': 'Apple 38-core GPU',\n",
    "    'm4 max': 'Apple 38-core GPU',\n",
    "}\n",
    "\n",
    "def get_apple_gpu_for_cpu(cpu_name):\n",
    "    \"\"\"Map Apple Silicon CPU to appropriate GPU benchmark entry.\"\"\"\n",
    "    if not cpu_name or pd.isna(cpu_name):\n",
    "        return None\n",
    "    cpu_lower = str(cpu_name).lower()\n",
    "    \n",
    "    # Check from most specific to least specific\n",
    "    for pattern, gpu_name in sorted(APPLE_GPU_MAP.items(), key=lambda x: -len(x[0])):\n",
    "        if pattern in cpu_lower:\n",
    "            return gpu_name\n",
    "    return None\n",
    "\n",
    "# CPU-based GPU inference for CPUs with no gpu_name assigned\n",
    "def infer_gpu_from_cpu(cpu_name):\n",
    "    \"\"\"Infer GPU from CPU name when no gpu_name was assigned.\"\"\"\n",
    "    if not cpu_name or pd.isna(cpu_name):\n",
    "        return None\n",
    "    cpu_lower = str(cpu_name).lower()\n",
    "    \n",
    "    # Qualcomm Snapdragon - use Adreno GPUs\n",
    "    if 'snapdragon' in cpu_lower:\n",
    "        if '8cx' in cpu_lower or '8c' in cpu_lower:\n",
    "            return 'Adreno 680'  # High-end Snapdragon\n",
    "        elif '7c' in cpu_lower:\n",
    "            return 'Adreno 618'  # Mid-range\n",
    "        else:\n",
    "            return 'Adreno 618'  # Default Snapdragon\n",
    "    \n",
    "    # Intel Core 2 Duo / Core Duo - GMA integrated graphics\n",
    "    if 'core 2 duo' in cpu_lower or 'core duo' in cpu_lower:\n",
    "        return 'Intel GMA 4500MHD'  # Common integrated GPU for this era\n",
    "    \n",
    "    # Intel Celeron (old)\n",
    "    if 'celeron' in cpu_lower and ('t3' in cpu_lower or 't1' in cpu_lower):\n",
    "        return 'Intel GMA 4500MHD'\n",
    "    \n",
    "    # Generic Intel Core without model (like \"INTEL CORE 620\") - old laptop\n",
    "    if 'intel' in cpu_lower and 'core' in cpu_lower:\n",
    "        return 'Intel GMA 4500MHD'  # Assume old integrated graphics\n",
    "    \n",
    "    return None\n",
    "\n",
    "# GPU mapping threshold\n",
    "GPU_MATCH_THRESHOLD = 50\n",
    "\n",
    "# Initialize new columns\n",
    "new_data['gpu_match_score'] = np.nan\n",
    "new_data['gpu_g3d_mark'] = np.nan\n",
    "new_data['gpu_g2d_mark'] = np.nan\n",
    "new_data['gpu_tdp'] = np.nan\n",
    "\n",
    "# Map GPUs\n",
    "dedicated_matched = 0\n",
    "integrated_matched = 0\n",
    "apple_matched = 0\n",
    "inferred_matched = 0\n",
    "gpu_unmatched = 0\n",
    "gpu_scores = []\n",
    "\n",
    "# Find the CPU column\n",
    "cpu_col = None\n",
    "for col in ['CPU', 'cpu', 'cpu_name', 'Cpu']:\n",
    "    if col in new_data.columns:\n",
    "        cpu_col = col\n",
    "        break\n",
    "\n",
    "for idx, row in new_data.iterrows():\n",
    "    dedicated = row.get('DEDICATED_GPU')\n",
    "    cpu_name = row.get(cpu_col) if cpu_col else None\n",
    "    \n",
    "    # Determine which GPU to look up\n",
    "    if pd.isna(dedicated) or str(dedicated).strip() == '':\n",
    "        # No dedicated GPU - look up the integrated GPU from gpu_name column\n",
    "        gpu_to_match = row.get('gpu_name')\n",
    "        is_dedicated = False\n",
    "    else:\n",
    "        # Has dedicated GPU - match the dedicated GPU\n",
    "        gpu_to_match = dedicated\n",
    "        is_dedicated = True\n",
    "    \n",
    "    # If no gpu_name, try to infer from CPU\n",
    "    if (pd.isna(gpu_to_match) or str(gpu_to_match).strip() == '' or str(gpu_to_match).strip() == 'NA') and not is_dedicated:\n",
    "        inferred_gpu = infer_gpu_from_cpu(cpu_name)\n",
    "        if inferred_gpu:\n",
    "            gpu_to_match = inferred_gpu\n",
    "            new_data.at[idx, 'gpu_name'] = inferred_gpu\n",
    "    \n",
    "    # Skip if still no GPU to match\n",
    "    if pd.isna(gpu_to_match) or str(gpu_to_match).strip() == '' or str(gpu_to_match).strip() == 'NA':\n",
    "        gpu_unmatched += 1\n",
    "        continue\n",
    "    \n",
    "    # Special handling for generic \"Apple GPU\"\n",
    "    if 'apple gpu' in str(gpu_to_match).lower():\n",
    "        apple_gpu = get_apple_gpu_for_cpu(cpu_name)\n",
    "        if apple_gpu and apple_gpu.lower() in gpu_by_name:\n",
    "            g = gpu_by_name[apple_gpu.lower()]\n",
    "            new_data.at[idx, 'gpu_name'] = g['gpu_name']\n",
    "            new_data.at[idx, 'gpu_match_score'] = 100\n",
    "            new_data.at[idx, 'gpu_g3d_mark'] = g.get('g3d_mark', None)\n",
    "            new_data.at[idx, 'gpu_g2d_mark'] = g.get('g2d_mark', None)\n",
    "            new_data.at[idx, 'gpu_tdp'] = g.get('tdp(w)', None)\n",
    "            apple_matched += 1\n",
    "            gpu_scores.append(100)\n",
    "            continue\n",
    "    \n",
    "    # Normalize and match GPU\n",
    "    norm_gpu = normalize_gpu(gpu_to_match)\n",
    "    if not norm_gpu:\n",
    "        gpu_unmatched += 1\n",
    "        continue\n",
    "    \n",
    "    match = best_gpu_match(norm_gpu, gpu_norms)\n",
    "    if match:\n",
    "        choice, score, match_idx = match\n",
    "        gpu_scores.append(score)\n",
    "        if score >= GPU_MATCH_THRESHOLD:\n",
    "            g = gpu_data[match_idx]\n",
    "            # Update gpu_name only if it's a dedicated GPU or was inferred\n",
    "            if is_dedicated:\n",
    "                new_data.at[idx, 'gpu_name'] = g['gpu_name']\n",
    "                dedicated_matched += 1\n",
    "            else:\n",
    "                integrated_matched += 1\n",
    "            # Always fill the benchmark attributes\n",
    "            new_data.at[idx, 'gpu_match_score'] = score\n",
    "            new_data.at[idx, 'gpu_g3d_mark'] = g.get('g3d_mark', None)\n",
    "            new_data.at[idx, 'gpu_g2d_mark'] = g.get('g2d_mark', None)\n",
    "            new_data.at[idx, 'gpu_tdp'] = g.get('tdp(w)', None)\n",
    "        else:\n",
    "            # Low score - keep the inferred/assigned name but mark as unmatched\n",
    "            gpu_unmatched += 1\n",
    "    else:\n",
    "        gpu_unmatched += 1\n",
    "        gpu_scores.append(0)\n",
    "\n",
    "# Report\n",
    "avg_gpu_score = sum(gpu_scores) / len(gpu_scores) if gpu_scores else 0\n",
    "print(f'GPU Mapping: Dedicated: {dedicated_matched}, Integrated: {integrated_matched}, Apple: {apple_matched}, Unmatched: {gpu_unmatched}')\n",
    "print(f'Avg score: {avg_gpu_score:.1f}')\n",
    "print(f'\\nSample gpu_name values after mapping:')\n",
    "print(new_data['gpu_name'].dropna().value_counts().head(15))\n",
    "print(f'\\nGPU benchmark columns filled: {new_data[\"gpu_g3d_mark\"].notna().sum()} rows')\n",
    "\n",
    "# Show remaining unmatched\n",
    "still_unmatched = new_data[new_data['gpu_g3d_mark'].isna()]\n",
    "if len(still_unmatched) > 0:\n",
    "    print(f'\\nRemaining unmatched ({len(still_unmatched)} rows):')\n",
    "    print(still_unmatched[['gpu_name', cpu_col]].head(20) if cpu_col else still_unmatched[['gpu_name']].head(20))\n",
    "\n",
    "# Export\n",
    "new_data.to_csv('data_with_cpus_gpus.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
